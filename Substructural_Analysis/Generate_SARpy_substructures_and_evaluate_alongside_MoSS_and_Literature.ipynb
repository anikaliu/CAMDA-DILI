{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print('\\n %s structures matched' % c)? (SARpy.py, line 249)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/anaconda3/envs/myp3env/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3267\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-b5d52acdbd91>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import SARpy\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/psrw2/OneDrive - University Of Cambridge/GitHub/CAMDA-DILI/Substructural_Analysis/SARpy.py\"\u001b[0;36m, line \u001b[0;32m249\u001b[0m\n\u001b[0;31m    print '\\n %s structures matched' % c\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print('\\n %s structures matched' % c)?\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SARpy\n",
    "import operator\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.multitest as smm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = pd.read_csv(\"./Most_No_DILIConcern_Dataset_for_SAs.csv\")\n",
    "structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Compound_Name = structures['Compound Name'].tolist()\n",
    "SMILES = structures.loc[:,['SMILES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label (x):\n",
    "    if x == 'vMost-DILI-Concern':\n",
    "        return 1\n",
    "    if x == 'vNo-DILI-Concern':\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES['DILI_LABEL'] = structures['vDILIConcern'].apply(label)\n",
    "SMILES.to_csv(\"Data_for_SARpy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Data_for_SARpy.csv\")\n",
    "x = data[['SMILES','DILI_LABEL']]\n",
    "y = data['DILI_LABEL']\n",
    "x.to_csv(\"Train_Data_for_SARpy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = SARpy.Filter('DILI_LABEL',0, operator.eq)\n",
    "filt2 = SARpy.Filter('DILI_LABEL',1, operator.eq)\n",
    "dictionary = {'INACTIVE':filt1, 'ACTIVE':filt2}\n",
    "dataset = SARpy.loadDataset(\"Train_Data_for_SARpy.csv\",'csv',dictionary,'SMILES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARpy.fragmentize(dataset,2,15)\n",
    "rules = SARpy.extract(dataset, minHits = 5,minLR = 1, minPrecision = 0) # 'OPTIMAL'\n",
    "SARpy.saveSmarts(rules,'ruleset.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Way to get SARpy substructures for DILI (not for noDILI)\n",
    "\n",
    "accuracy_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "SMARTS_list = []\n",
    "LR_list = []\n",
    "Target_list = []\n",
    "test_comp_matched_list = []\n",
    "for rule in rules:\n",
    "    myrules = [rule,rule]\n",
    "    SARpy.saveSmarts(myrules,'ruleset.txt')\n",
    "    alerts = pd.read_csv(\"ruleset.txt\", sep='\\t')\n",
    "    alerts = alerts.ix[1]\n",
    "    #print(alerts)\n",
    "    SMARTS = alerts['SMARTS']\n",
    "    TARGET = alerts['Target']\n",
    "    Target_list.append(TARGET)\n",
    "#    if TARGET == 'ACTIVE':\n",
    "        \n",
    "    \n",
    "    LR = alerts['Training LR']\n",
    "    pred = SARpy.predict(myrules,dataset) #dataset_test\n",
    "    if pred >= 1:\n",
    "        accuracy,sensitivity,specificity = SARpy.validate(dataset) #\n",
    "            #print(accuracy,sensitivity,specificity,LR, SMARTS)\n",
    "        pred = (pred/167.0)*100\n",
    "        test_comp_matched_list.append((pred))\n",
    "        accuracy_list.append(accuracy)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        SMARTS_list.append(SMARTS)\n",
    "        LR_list.append(LR)\n",
    "            \n",
    "            \n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        \n",
    "df = pd.DataFrame()\n",
    "df['SMARTS'] = SMARTS_list\n",
    "df['TYPE'] = Target_list\n",
    "df['Accuracy'] = accuracy_list\n",
    "df['Sensitivity'] = sensitivity_list\n",
    "df['Specificity'] = specificity_list\n",
    "df['LR_training'] = LR_list\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Source'] = 'SARpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add MOSS substructures (from KNIME workflow)\n",
    "MOSS_SS_Active = pd.read_csv(\"./moss_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOSS_SS_Active['Fragment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MOSS_SS_Active = MOSS_SS_Active['Fragment'].tolist()\n",
    "\n",
    "MOSS_SS_Active = dict(zip(MOSS_SS_Active, ['ACTIVE'] * len(MOSS_SS_Active)))\n",
    "\n",
    "\n",
    "MOSS_SS_Active = pd.DataFrame(MOSS_SS_Active.items(), columns=['SMARTS', 'TYPE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOSS_SS = pd.concat([MOSS_SS_Active,MOSS_SS_Inactive])\n",
    "MOSS_SS = MOSS_SS_Active\n",
    "MOSS_SS['Source'] = 'MOSS'\n",
    "df = pd.concat([MOSS_SS,df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add literature SMARTS\n",
    "\n",
    "Literature_SMARTS = pd.read_csv(\"./Literature_SMARTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Literature_SMARTS['Source'] = 'Hewitt et al. (2013)'\n",
    "Literature_SMARTS['Source'][16:] = \"Liu et al. (2015)\"\n",
    "Literature_SMARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([Literature_SMARTS,df])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Drugbank approved\n",
    "\n",
    "Drugbank = pd.read_csv(\"./DrugBank_5_1_4_approved.csv\")\n",
    "Drugbank = Drugbank[pd.notnull(Drugbank['SMILES'])]\n",
    "Drugbank.drop_duplicates(subset='SMILES', keep=\"first\",inplace=True)\n",
    "Drugbank_comps = Drugbank['SMILES'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x.to_csv(\"../CAMDA_Model_Data/MCNC_for_Substructures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tabulate results (throughout this script PPV is used as a synomym for precision)\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.DataFrame(index = x['SMILES'])\n",
    "df2['DILI_LABEL'] = x['DILI_LABEL'].to_list()\n",
    "df2['Compound_Name'] = Compound_Name\n",
    "\n",
    "substructure_stats = pd.DataFrame()\n",
    "\n",
    "#generate FPs\n",
    "smis = list(df2.index)\n",
    "\n",
    "mols = [Chem.MolFromSmiles(smile) for smile in smis]\n",
    "\n",
    "fps_bit = [list(Chem.GetMorganFingerprintAsBitVect(mol,2, nBits=2048)) for mol in mols]\n",
    "\n",
    "\n",
    "h = 0 # count TPs\n",
    "l = 0 # count FPs\n",
    "smart_list = []\n",
    "PPV_list = []\n",
    "percent_hits = []\n",
    "labels = []\n",
    "sources = []\n",
    "odds_ratio_list = []\n",
    "p_value_list = []\n",
    "perc_hit_list = []\n",
    "type_list = []\n",
    "DB_count_list = []\n",
    "for index, row in df.iterrows():\n",
    "    i=0\n",
    "    j=0\n",
    "    k=0\n",
    "    z=0\n",
    "    t=0\n",
    "    c=0\n",
    "    v=0\n",
    "    DB_count = 0\n",
    "    hit_list = []\n",
    "    smart = row['SMARTS']\n",
    "    TYPE = row['TYPE']\n",
    "    source = row['Source']\n",
    "    \n",
    " \n",
    "    if TYPE == 'ACTIVE':\n",
    "#        for DB in Drugbank_comps:\n",
    "#            m = Chem.MolFromSmiles(DB)\n",
    "#            patt = Chem.MolFromSmarts(smart)\n",
    "#            if m.HasSubstructMatch(patt):\n",
    "#                DB_count+=1\n",
    "#            else:\n",
    "#                continue\n",
    "#        DB_count_list.append(DB_count)\n",
    "    \n",
    "    \n",
    "        for index, row in x.iterrows():\n",
    "            label = row['DILI_LABEL']\n",
    "            a = row['SMILES']   \n",
    "            m = Chem.MolFromSmiles(a)\n",
    "            patt = Chem.MolFromSmarts(smart)\n",
    "            if m.HasSubstructMatch(patt) is True:\n",
    "                i+=1\n",
    "            if m.HasSubstructMatch(patt) is True and label == 1: # True positive\n",
    "                #print('TP')\n",
    "                j+=1\n",
    "            if m.HasSubstructMatch(patt) is True and label == 0: # False positive\n",
    "                #print('FP')\n",
    "                k+=1\n",
    "            if m.HasSubstructMatch(patt) is True:\n",
    "                hit_list.append(1)\n",
    "            if m.HasSubstructMatch(patt) is False:\n",
    "                hit_list.append('0')\n",
    "                \n",
    "                \n",
    "            if m.HasSubstructMatch(patt) is True and label == 1: # compound is DILI and has SS == TP\n",
    "                z+=1\n",
    "            if m.HasSubstructMatch(patt) is False and label == 1: # compound is DILI and does not have SS == FN\n",
    "                t+=1\n",
    "            if m.HasSubstructMatch(patt) is True and label == 0: # compound is noDILI and has SS == FP\n",
    "                c+=1\n",
    "            if m.HasSubstructMatch(patt) is False and label == 0: # compound is noDILI and does not have SS == TN\n",
    "                v+=1    \n",
    "                \n",
    "        #if source == 'Hewitt et al. (2013)':\n",
    "           # sources.append(0)\n",
    "        if source == 'Liu et al. (2015)':\n",
    "            sources.append(1)\n",
    "        elif source == 'SARpy':\n",
    "            sources.append(2)\n",
    "        elif source == 'MOSS':\n",
    "            sources.append(3)\n",
    "        else:\n",
    "            continue\n",
    "                \n",
    "            \n",
    "        #print(j)\n",
    "        #print(len(x.loc[x['DILI_LABEL']==1]))\n",
    "        perc_hits = (j*1.0/len(x.loc[x['DILI_LABEL']==1]))*100.0\n",
    "        percent_hits.append(perc_hits)\n",
    "        labels.append(0)\n",
    "\n",
    "    \n",
    "        if ((j+k) == 0):\n",
    "            PPV = 0\n",
    "        else:\n",
    "\n",
    "            PPV = (j/(j+k*1.0))\n",
    "            print('{0:.3f}'.format(PPV), smart, perc_hits) # positive predictive value (most important for substructures to classify DILI)\n",
    "\n",
    "\n",
    "        #PPV_other = z/(z+c)\n",
    "        print(PPV,j,z,k,c)\n",
    "\n",
    "        PPV_list.append(PPV)\n",
    "\n",
    "        # Fisher one-sided test\n",
    "\n",
    "        oddsratio, pvalue = stats.fisher_exact([[z, c], [t, v]], alternative='greater')\n",
    "\n",
    "        odds_ratio_list.append(oddsratio)\n",
    "        p_value_list.append(pvalue)\n",
    "        perc_hit_list.append(perc_hits)\n",
    "        type_list.append(TYPE)\n",
    "        smart_list.append(smart)\n",
    "\n",
    "\n",
    "        if PPV >= 0.0: # filter for PPV\n",
    "\n",
    "            #df2[smart+'_'+TYPE] = hit_list\n",
    "            print(TYPE,i,j,k,pvalue,oddsratio,source,len(x['SMILES']),len(x.loc[x['DILI_LABEL']==1]),len(x.loc[x['DILI_LABEL']==0]))\n",
    "            #df2[smart] = hit_list\n",
    "\n",
    "            h+=j\n",
    "            l+=k\n",
    "            #PPV_list[smart] = PPV\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "substructure_stats = pd.DataFrame(index = smart_list)\n",
    "substructure_stats['PPV'] = PPV_list\n",
    "substructure_stats['Source'] = sources\n",
    "substructure_stats['odds_ratio'] = odds_ratio_list\n",
    "substructure_stats['p_value'] = p_value_list\n",
    "substructure_stats['perc_hits'] = perc_hit_list\n",
    "substructure_stats['type'] = type_list\n",
    "\n",
    "\n",
    "print(h,l) # number of TPs and FPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(smart_list),len(PPV_list),len(sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_stats.loc[substructure_stats['Source']==3]['PPV'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rej, pval_corr = smm.multipletests(p_value_list, method='fdr_bh')[:2]\n",
    "substructure_stats['BH_Adjusted_p_value'] = list(pval_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substructure_stats.to_csv(\"./Structural_alerts.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
